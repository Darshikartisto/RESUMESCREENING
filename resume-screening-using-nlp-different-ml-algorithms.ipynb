{"cells":[{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"background-color:#1E90FF; color:white\" style=\"font-family: Cambria\" align=\"center\">Resume Screening using NLP+Different ML Algorithms</h1> "]},{"cell_type":"markdown","metadata":{},"source":["**A summary of resume screening:**\n","\n","- **1. Definition**: Resume screening is the process of determining whether a candidate is qualified for a role based his or her education, experience, and other information captured on their resume.\n","\n","- **2. How to screen resumes**: First, screen resumes based on the job’s minimum qualifications. Second, screen resumes based on the job’s preferred qualifications. Third, screen resumes based on the shortlist of candidates you want to move onto the interview phase.\n","\n","- **3. The challenges recruiters face while screening resumes**: The high volume of resumes received – up to 88% of them are unqualified – greatly increases time to fill. Recruiters face increased pressure to show quality of hire but lack tools to link their resume screening to post-hire metrics.\n","\n","- **4. Tech innovations in resume screening**: Intelligent resume screening by using AI to learn from historical hiring decisions to improve quality of hire and reduce employee turnover.\n","\n","<a href=\"https://ideal.com/resume-screening/#:~:text=Resume%20screening%20is%20the%20process,candidate%20based%20on%20their%20resume\" target=\"_blank\" rel=\"noopener noreferrer\">Source</a> \n","\n","In this project, machine learning models is developed for the Resume Screening task."]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n","        <h3 class=\"list-group-item list-group-item-action active\" style=\"background-color:#1E90FF; color:white\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\" style=\"font-family: Cambria\">Notebook Content</h3>     \n","        <a class=\"list-group-item list-group-item-action list-group-item-info\" data-toggle=\"list\" href=\"#import\" role=\"tab\" aria-controls=\"profile\" style=\"font-family: Cambria\">Importing Basic Libraries and Loading Dataset<span class=\"badge badge-primary badge-pill\" style=\"background-color:#1E90FF; color:white\">1</span></a>      \n","        <a class=\"list-group-item list-group-item-action list-group-item-info\" data-toggle=\"list\" href=\"#understand\" role=\"tab\" aria-controls=\"profile\" style=\"font-family: Cambria\">Understanding Dataset<span class=\"badge badge-primary badge-pill\" style=\"background-color:#1E90FF; color:white\">2</span></a>\n","        <a class=\"list-group-item list-group-item-action list-group-item-info\" data-toggle=\"list\" href=\"#prep\" role=\"tab\" aria-controls=\"profile\" style=\"font-family: Cambria\">Preprocessing<span class=\"badge badge-primary badge-pill\" style=\"background-color:#1E90FF; color:white\">3</span></a>  \n","        <a class=\"list-group-item list-group-item-action list-group-item-info\" data-toggle=\"list\" href=\"#model\" role=\"tab\" aria-controls=\"profile\" style=\"font-family: Cambria\">Building Models<span class=\"badge badge-primary badge-pill\" style=\"background-color:#1E90FF; color:white\">4</span></a> \n","        <a class=\"list-group-item list-group-item-action list-group-item-info\" data-toggle=\"list\" href=\"#cross\" role=\"tab\" aria-controls=\"profile\" style=\"font-family: Cambria\">Cross Validation for Models<span class=\"badge badge-primary badge-pill\" style=\"background-color:#1E90FF; color:white\">5</span></a> "]},{"cell_type":"markdown","metadata":{},"source":["<a id='import'></a>\n","<h1 style=\"background-color:#1E90FF; color:white\" style=\"font-family: Cambria\">Importing Basic Libraries and Loading Dataset</h1> "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'c:\\Users\\darshika\\Desktop\\MANIM\\.conda\\python.exe' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -p c:\\Users\\darshika\\Desktop\\MANIM\\.conda ipykernel --update-deps --force-reinstall'"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["df= pd.read_csv('UpdatedResumeDataSet.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["<a id='understand'></a>\n","<h1 style=\"background-color:#1E90FF; color:white\" style=\"font-family: Cambria\">Understanding Dataset</h1> "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['Category'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['Category'].nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["categories = df['Category'].value_counts().reset_index()\n","categories"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(25,8))\n","plt.xticks(rotation=60)\n","# count plot on single categorical variable\n","sns.countplot(x ='Category', data= df, order= df['Category'].value_counts().index)\n"," \n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(25,8))\n","\n","#define Seaborn color palette to use\n","colors= sns.color_palette('bright')[0:5]\n","\n","#create pie chart\n","plt.pie(categories['Category'], labels= categories['index'], colors = colors, autopct='%.1f%%')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["<a id='prep'></a>\n","<h1 style=\"background-color:#1E90FF; color:white\" style=\"font-family: Cambria\">Preprocessing</h1> "]},{"cell_type":"markdown","metadata":{},"source":["Let's create a helper function to remove URLs, hashtags, mentions, special letters and punctuation\n","\n","Firstly, Let's add a new column for this:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df1= df.copy()\n","df1['cleaned_resume']= \"\"\n","df1"]},{"cell_type":"markdown","metadata":{},"source":["Function:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","def clean_function(resumeText):\n","    resumeText = re.sub('http\\S+\\s*', ' ', resumeText)  # remove URLs\n","    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc\n","    resumeText = re.sub('#\\S+', '', resumeText)  # remove hashtags\n","    resumeText = re.sub('@\\S+', '  ', resumeText)  # remove mentions\n","    resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n","    resumeText = re.sub(r'[^\\x00-\\x7f]',r' ', resumeText) \n","    resumeText = re.sub('\\s+', ' ', resumeText)  # remove extra whitespace\n","    return resumeText"]},{"cell_type":"markdown","metadata":{},"source":["Let's apply to columns:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df1['cleaned_resume'] = df1['Resume'].apply(lambda x: clean_function(x))\n","df1.head()"]},{"cell_type":"markdown","metadata":{},"source":["Let's encode the Category column:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","df2= df1.copy()\n","df2['Category']= LabelEncoder().fit_transform(df2['Category'])\n","df2.head()"]},{"cell_type":"markdown","metadata":{},"source":["Let's create wordcloud:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import nltk\n","from nltk.corpus import stopwords\n","import string\n","from wordcloud import WordCloud"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Stop words are generally the most common words in a language.\n","#English stop words from nltk:\n","SetOfStopWords= set(stopwords.words('english')+['``',\"''\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["totalWords= []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Sentences= df2['Resume'].values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cleanedSentences= \"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for records in Sentences:\n","    cleanedText= clean_function(records)\n","    cleanedSentences += cleanedText\n","    requiredWords = nltk.word_tokenize(cleanedText)\n","    for word in requiredWords:\n","        if word not in SetOfStopWords and word not in string.punctuation:\n","            totalWords.append(word)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["wordfreqdist = nltk.FreqDist(totalWords)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["wordfreqdist"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mostcommon = wordfreqdist.most_common(30)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mostcommon"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["WordCloud= WordCloud().generate(cleanedSentences)\n","plt.figure(figsize=(10,10))\n","plt.imshow(WordCloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["<a id='model'></a>\n","<h1 style=\"background-color:#1E90FF; color:white\" style=\"font-family: Cambria\">Building Models</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from scipy.sparse import hstack"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Text= df2['cleaned_resume'].values\n","Target= df2['Category'].values"]},{"cell_type":"markdown","metadata":{},"source":["Here we will preprocess and convert the ‘cleaned_resume’ column into vectors. We will be using the ‘Tf-Idf’ method to get the vectors:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_vectorizer = TfidfVectorizer(sublinear_tf=True, stop_words='english')\n","word_vectorizer.fit(Text)\n","WordFeatures= word_vectorizer.transform(Text)"]},{"cell_type":"markdown","metadata":{},"source":["We have ‘WordFeatures’ as vectors and ‘Target’ and target after this step."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["WordFeatures.shape"]},{"cell_type":"markdown","metadata":{},"source":["Let’s split the data into training and test set:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train,X_test,y_train,y_test= train_test_split(WordFeatures, Target, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(X_train.shape)\n","print(X_test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["We have trained and tested the data and now let’s build the models:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.multiclass import OneVsRestClassifier\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import RandomForestClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["models = {\n","    'K-Nearest Neighbors' : KNeighborsClassifier(),\n","    'Logistic Regression' : LogisticRegression(),\n","    'Support Vector Machine' : SVC(),\n","    'Random Forest' : RandomForestClassifier()    \n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_list=[]\n","for model in models.values():\n","    model_list.append(OneVsRestClassifier(model))\n","model_list"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in model_list:\n","    i.fit(X_train, y_train)\n","    print(f'{i} trained')\n","\n","print(\"*\"*60)\n","print(\"all models trained\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for count, value in enumerate(model_list):\n","    print(f\"Accuracy of {value} on training set :\", model_list[count].score(X_train, y_train))\n","    print(f\"Accuracy of {value} on test set :\", model_list[count].score(X_test, y_test))\n","    print(\"*\"*100)\n","\n","print(\"all scores calculated\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import confusion_matrix as CM\n","from sklearn.metrics import classification_report\n","\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for count, value in enumerate(model_list):\n","    print(f'{value} classification report')\n","    print(\"-\"*80)\n","    print(classification_report(y_test, model_list[count].predict(X_test)))\n","    print(\"*\"*100)\n","    print(\" \")"]},{"cell_type":"markdown","metadata":{},"source":["<a id='cross'></a>\n","<h1 style=\"background-color:#1E90FF; color:white\" style=\"font-family: Cambria\">Cross Validation for Models</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import cross_val_score, KFold"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results = {}\n","\n","kf = KFold(n_splits= 10)\n","\n","for count, value in enumerate(model_list):\n","    result = cross_val_score(model_list[count], X_train, y_train, scoring= 'accuracy', cv= kf)\n","    results[value] = result"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"r2 scores\")\n","print(\"*********************************\")\n","for name, result in results.items():\n","   \n","    print(f'{name} : {round(np.mean(result),3)}')\n","    print(\"----------------\")"]},{"cell_type":"markdown","metadata":{},"source":["This project, it is showed how different machine learning algorithms could be applied for building a system such as a resume screening. \n","\n","The models just classified almost 1000 resumes in a few minutes into their respective categories with 99% accuracy."]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"color:white; font-size:125%; text-align:left; display:fill; border-radius:5px; background-color:#1E90FF; overflow:hidden\">Thanks for reading. I hope you enjoy it and that it was helpful to you.<br>Please don't forget to follow me and give an upvote on</br>\n","👇👇👇\n","</div>\n","\n","**<a href=\"https://www.kaggle.com/drindeng/\" target=\"_blank\" rel=\"noopener noreferrer\">[Kaggle]</a> | \n","<a href=\"https://github.com/drindeng\" target=\"_blank\" rel=\"noopener noreferrer\">[GitHub]</a> |\n","<a href=\"https://www.linkedin.com/in/turgay-turker/\" target=\"_blank\" rel=\"noopener noreferrer\">[Linkedin]</a>**"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
