{"cells":[{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"background-color:#1E90FF; color:white\" style=\"font-family: Cambria\" align=\"center\">Resume Screening using NLP+Different ML Algorithms</h1> "]},{"cell_type":"markdown","metadata":{},"source":["**A summary of resume screening:**\n","\n","- **1. Definition**: Resume screening is the process of determining whether a candidate is qualified for a role based his or her education, experience, and other information captured on their resume.\n","\n","- **2. How to screen resumes**: First, screen resumes based on the jobâ€™s minimum qualifications. Second, screen resumes based on the jobâ€™s preferred qualifications. Third, screen resumes based on the shortlist of candidates you want to move onto the interview phase.\n","\n","- **3. The challenges recruiters face while screening resumes**: The high volume of resumes received â€“ up to 88% of them are unqualified â€“ greatly increases time to fill. Recruiters face increased pressure to show quality of hire but lack tools to link their resume screening to post-hire metrics.\n","\n","- **4. Tech innovations in resume screening**: Intelligent resume screening by using AI to learn from historical hiring decisions to improve quality of hire and reduce employee turnover.\n","\n","<a href=\"https://ideal.com/resume-screening/#:~:text=Resume%20screening%20is%20the%20process,candidate%20based%20on%20their%20resume\" target=\"_blank\" rel=\"noopener noreferrer\">Source</a> \n","\n","In this project, machine learning models is developed for the Resume Screening task."]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n","        <h3 class=\"list-group-item list-group-item-action active\" style=\"background-color:#1E90FF; color:white\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\" style=\"font-family: Cambria\">Notebook Content</h3>     \n","        <a class=\"list-group-item list-group-item-action list-group-item-info\" data-toggle=\"list\" href=\"#import\" role=\"tab\" aria-controls=\"profile\" style=\"font-family: Cambria\">Importing Basic Libraries and Loading Dataset<span class=\"badge badge-primary badge-pill\" style=\"background-color:#1E90FF; color:white\">1</span></a>      \n","        <a class=\"list-group-item list-group-item-action list-group-item-info\" data-toggle=\"list\" href=\"#understand\" role=\"tab\" aria-controls=\"profile\" style=\"font-family: Cambria\">Understanding Dataset<span class=\"badge badge-primary badge-pill\" style=\"background-color:#1E90FF; color:white\">2</span></a>\n","        <a class=\"list-group-item list-group-item-action list-group-item-info\" data-toggle=\"list\" href=\"#prep\" role=\"tab\" aria-controls=\"profile\" style=\"font-family: Cambria\">Preprocessing<span class=\"badge badge-primary badge-pill\" style=\"background-color:#1E90FF; color:white\">3</span></a>  \n","        <a class=\"list-group-item list-group-item-action list-group-item-info\" data-toggle=\"list\" href=\"#model\" role=\"tab\" aria-controls=\"profile\" style=\"font-family: Cambria\">Building Models<span class=\"badge badge-primary badge-pill\" style=\"background-color:#1E90FF; color:white\">4</span></a> \n","        <a class=\"list-group-item list-group-item-action list-group-item-info\" data-toggle=\"list\" href=\"#cross\" role=\"tab\" aria-controls=\"profile\" style=\"font-family: Cambria\">Cross Validation for Models<span class=\"badge badge-primary badge-pill\" style=\"background-color:#1E90FF; color:white\">5</span></a> "]},{"cell_type":"markdown","metadata":{},"source":["<a id='import'></a>\n","<h1 style=\"background-color:#1E90FF; color:white\" style=\"font-family: Cambria\">Importing Basic Libraries and Loading Dataset</h1> "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'c:\\Users\\darshika\\Desktop\\MANIM\\.conda\\python.exe' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -p c:\\Users\\darshika\\Desktop\\MANIM\\.conda ipykernel --update-deps --force-reinstall'"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["df= pd.read_csv('UpdatedResumeDataSet.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["<a id='understand'></a>\n","<h1 style=\"background-color:#1E90FF; color:white\" style=\"font-family: Cambria\">Understanding Dataset</h1> "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['Category'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['Category'].nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["categories = df['Category'].value_counts().reset_index()\n","categories"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(25,8))\n","plt.xticks(rotation=60)\n","# count plot on single categorical variable\n","sns.countplot(x ='Category', data= df, order= df['Category'].value_counts().index)\n"," \n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(25,8))\n","\n","#define Seaborn color palette to use\n","colors= sns.color_palette('bright')[0:5]\n","\n","#create pie chart\n","plt.pie(categories['Category'], labels= categories['index'], colors = colors, autopct='%.1f%%')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["<a id='prep'></a>\n","<h1 style=\"background-color:#1E90FF; color:white\" style=\"font-family: Cambria\">Preprocessing</h1> "]},{"cell_type":"markdown","metadata":{},"source":["Let's create a helper function to remove URLs, hashtags, mentions, special letters and punctuation\n","\n","Firstly, Let's add a new column for this:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df1= df.copy()\n","df1['cleaned_resume']= \"\"\n","df1"]},{"cell_type":"markdown","metadata":{},"source":["Function:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","def clean_function(resumeText):\n","    resumeText = re.sub('http\\S+\\s*', ' ', resumeText)  # remove URLs\n","    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc\n","    resumeText = re.sub('#\\S+', '', resumeText)  # remove hashtags\n","    resumeText = re.sub('@\\S+', '  ', resumeText)  # remove mentions\n","    resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n","    resumeText = re.sub(r'[^\\x00-\\x7f]',r' ', resumeText) \n","    resumeText = re.sub('\\s+', ' ', resumeText)  # remove extra whitespace\n","    return resumeText"]},{"cell_type":"markdown","metadata":{},"source":["Let's apply to columns:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df1['cleaned_resume'] = df1['Resume'].apply(lambda x: clean_function(x))\n","df1.head()"]},{"cell_type":"markdown","metadata":{},"source":["Let's encode the Category column:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","df2= df1.copy()\n","df2['Category']= LabelEncoder().fit_transform(df2['Category'])\n","df2.head()"]},{"cell_type":"markdown","metadata":{},"source":["Let's create wordcloud:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import nltk\n","from nltk.corpus import stopwords\n","import string\n","from wordcloud import WordCloud"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Stop words are generally the most common words in a language.\n","#English stop words from nltk:\n","SetOfStopWords= set(stopwords.words('english')+['``',\"''\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["totalWords= []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Sentences= df2['Resume'].values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cleanedSentences= \"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for records in Sentences:\n","    cleanedText= clean_function(records)\n","    cleanedSentences += cleanedText\n","    requiredWords = nltk.word_tokenize(cleanedText)\n","    for word in requiredWords:\n","        if word not in SetOfStopWords and word not in string.punctuation:\n","            totalWords.append(word)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["wordfreqdist = nltk.FreqDist(totalWords)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["wordfreqdist"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mostcommon = wordfreqdist.most_common(30)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mostcommon"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["WordCloud= WordCloud().generate(cleanedSentences)\n","plt.figure(figsize=(10,10))\n","plt.imshow(WordCloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["<a id='model'></a>\n","<h1 style=\"background-color:#1E90FF; color:white\" style=\"font-family: Cambria\">Building Models</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from scipy.sparse import hstack"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Text= df2['cleaned_resume'].values\n","Target= df2['Category'].values"]},{"cell_type":"markdown","metadata":{},"source":["Here we will preprocess and convert the â€˜cleaned_resumeâ€™ column into vectors. We will be using the â€˜Tf-Idfâ€™ method to get the vectors:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_vectorizer = TfidfVectorizer(sublinear_tf=True, stop_words='english')\n","word_vectorizer.fit(Text)\n","WordFeatures= word_vectorizer.transform(Text)"]},{"cell_type":"markdown","metadata":{},"source":["We have â€˜WordFeaturesâ€™ as vectors and â€˜Targetâ€™ and target after this step."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["WordFeatures.shape"]},{"cell_type":"markdown","metadata":{},"source":["Letâ€™s split the data into training and test set:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train,X_test,y_train,y_test= train_test_split(WordFeatures, Target, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(X_train.shape)\n","print(X_test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["We have trained and tested the data and now letâ€™s build the models:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.multiclass import OneVsRestClassifier\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import RandomForestClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["models = {\n","    'K-Nearest Neighbors' : KNeighborsClassifier(),\n","    'Logistic Regression' : LogisticRegression(),\n","    'Support Vector Machine' : SVC(),\n","    'Random Forest' : RandomForestClassifier()    \n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_list=[]\n","for model in models.values():\n","    model_list.append(OneVsRestClassifier(model))\n","model_list"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in model_list:\n","    i.fit(X_train, y_train)\n","    print(f'{i} trained')\n","\n","print(\"*\"*60)\n","print(\"all models trained\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for count, value in enumerate(model_list):\n","    print(f\"Accuracy of {value} on training set :\", model_list[count].score(X_train, y_train))\n","    print(f\"Accuracy of {value} on test set :\", model_list[count].score(X_test, y_test))\n","    print(\"*\"*100)\n","\n","print(\"all scores calculated\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import confusion_matrix as CM\n","from sklearn.metrics import classification_report\n","\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for count, value in enumerate(model_list):\n","    print(f'{value} classification report')\n","    print(\"-\"*80)\n","    print(classification_report(y_test, model_list[count].predict(X_test)))\n","    print(\"*\"*100)\n","    print(\" \")"]},{"cell_type":"markdown","metadata":{},"source":["<a id='cross'></a>\n","<h1 style=\"background-color:#1E90FF; color:white\" style=\"font-family: Cambria\">Cross Validation for Models</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import cross_val_score, KFold"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results = {}\n","\n","kf = KFold(n_splits= 10)\n","\n","for count, value in enumerate(model_list):\n","    result = cross_val_score(model_list[count], X_train, y_train, scoring= 'accuracy', cv= kf)\n","    results[value] = result"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"r2 scores\")\n","print(\"*********************************\")\n","for name, result in results.items():\n","   \n","    print(f'{name} : {round(np.mean(result),3)}')\n","    print(\"----------------\")"]},{"cell_type":"markdown","metadata":{},"source":["This project, it is showed how different machine learning algorithms could be applied for building a system such as a resume screening. \n","\n","The models just classified almost 1000 resumes in a few minutes into their respective categories with 99% accuracy."]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"color:white; font-size:125%; text-align:left; display:fill; border-radius:5px; background-color:#1E90FF; overflow:hidden\">Thanks for reading. I hope you enjoy it and that it was helpful to you.<br>Please don't forget to follow me and give an upvote on</br>\n","ðŸ‘‡ðŸ‘‡ðŸ‘‡\n","</div>\n","\n","**<a href=\"https://www.kaggle.com/drindeng/\" target=\"_blank\" rel=\"noopener noreferrer\">[Kaggle]</a> | \n","<a href=\"https://github.com/drindeng\" target=\"_blank\" rel=\"noopener noreferrer\">[GitHub]</a> |\n","<a href=\"https://www.linkedin.com/in/turgay-turker/\" target=\"_blank\" rel=\"noopener noreferrer\">[Linkedin]</a>**"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
